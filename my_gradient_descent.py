# -*- coding: utf-8 -*-
"""env_create

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CwgoS9XK8ZYfzL14O2UUuoi0bCCVqJ3r
"""

import pandas as pd

url = "https://raw.githubusercontent.com/aiken2/learning/main/Salary_Data.csv"
data = pd.read_csv(url)
data
# y = w*x + b
x = data["YearsExperience"]
y = data["Salary"]

def compute_gradient(x, y, w, b):
  #w= 10
  #b= 10
  #n= len(x)
  #w_gradient = (2*x*(w*x+b-y)).sum() / n
  #b_gradient = (2  *(w*x+b-y)).sum() / n
  #w_gradient = (2*x*(w*x+b-y)).mean()
  #b_gradient = (2  *(w*x+b-y)).mean()
  w_gradient = (x*(w*x+b-y)).mean()
  b_gradient = (  (w*x+b-y)).mean()
  return w_gradient, b_gradient

compute_gradient(x, y, 20, 10)

def compute_cost(x, y, w, b):
    y_pred = w*x + b
    cost = (y - y_pred)**2
    cost = cost.sum() / len(x)
    return cost

#w=0
#b=0
#learning_rate = 0.001

def gradient_descent(x, y ,  w_init, b_init, learning_rate, cost_function, gradient_function, run_iter , p_iter=1000):
  c_hist = []
  w_hist = []
  b_hist = []
  w=w_init
  b=b_init
  for i in range(run_iter):
    #w_gradient, b_gradient = compute_gradient(x, y, w, b)
    w_gradient, b_gradient = gradient_function(x, y, w, b)
    w= w-w_gradient*learning_rate
    b= b-b_gradient*learning_rate
    cost = cost_function(x, y, w, b)
    w_hist.append(w)
    b_hist.append(b)
    c_hist.append(cost)

    if i%p_iter==0 :
        print(f"Ieration {i:5} : Cost:{cost: .4e}, w: {w: .2e}, b: {b: .2e}, w_gradient: {w_gradient: .2e}, b_gradient: {b_gradient: .2e}")
  return w, b, w_hist, b_hist, c_hist
    #print(compute_cost(x, y, w, b))

w_init = 0
b_init = 0
learning_rate = 1.0e-3
run_iter = 20000
w_final, b_final, w_hist, b_hist, c_hist = gradient_descent(x, y ,  w_init, b_init, learning_rate, compute_cost, compute_gradient, run_iter , p_iter=1000)

import matplotlib.pyplot as plt
import numpy as np

#plt.plot(np.arange(0, 20000), c_hist)
plt.plot(np.arange(0, 100), c_hist[:100])
plt.title("iteration vs cost")
plt.xlabel("iteration")
plt.ylabel("cost")
plt.show()

print(f"最終w b=({w_final: .2e}, {b_final: .2e})")

print(f"年資3.5 {w_final*3.5 + b_final:.1f}K")
print(f"年資3.5 {w_final*5.9 + b_final:.1f}K")

# w=-100~100 b=-100~100 的 cost
# import numpy as np

ws = np.arange(-100, 101)
bs = np.arange(-100, 101)
costs = np.zeros((201, 201))
# costs = np.zeros((len(ws), len(bs)))

i = 0
for w in ws:
  j = 0
  for b in bs:
    cost = compute_cost(x, y, w, b)
    costs[i, j] = cost
    #print(i)
    #print(j)
    j = j + 1
  i = i + 1
costs = costs / 1000

!pip install wget
import wget

wget.download("https://raw.githubusercontent.com/aiken2/learning/main/ChineseFont.ttf")

import matplotlib as mpl
from matplotlib.font_manager import fontManager
# from matplotlib.ticker import MultipleLocator, FormatStrFormatter

fontManager.addfont("ChineseFont.ttf")
mpl.rc("font",family="ChineseFont")

#fig = plt.figure(figsize=(16, 6))
fig = plt.figure(figsize=(7, 7))

ax = plt.axes(projection="3d")
ax.view_init(20, -65)
ax.xaxis.set_pane_color((1.0,1.0,1.0))
ax.yaxis.set_pane_color((1.0,1.0,1.0))
ax.zaxis.set_pane_color((1.0,1.0,1.0))

b_grid, w_grid = np.meshgrid(bs, ws)
ax.plot_surface(w_grid, b_grid, costs, alpha=0.3) #
ax.set_title("w b 對應的 cost")
ax.set_xlabel("w")
ax.set_ylabel("b")
ax.set_zlabel("cost (K)")
w_index, b_index = np.where(costs == np.min(costs))
ax.scatter(ws[w_index], bs[b_index], costs[w_index,b_index], color="red", s=40)

ax.scatter(w_hist[0], b_hist[0], c_hist[0], color="green", s=40)
ax.plot(w_hist, b_hist, c_hist)

plt.show()


#ax.xaxis.set_major_locator(MultipleLocator(2))
#ax.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))

# b_grid, w_grid = np.meshgrid(bs, ws)
# what is meshgrid
# https://wangcyeming.github.io/2018/11/12/numpy_meshgrid/
# ax.plot_surface(w_grid, b_grid, costs, cmap="Spectral_r", alpha=0.7) #
# ax.plot_surface(w_grid, b_grid, costs, alpha=0.3) #
#ax.plot_wireframe(w_grid, b_grid, costs, color="black",alpha=0.1) #, cmap="Spectral_r"
# ax.set_title("w b 對應的 cost")
#ax.set_xlabel("w", fontsize=7, labelpad=3)
#ax.set_ylabel("b", fontsize=7, labelpad=3)
#ax.set_zlabel("cost (K)", fontsize=7, labelpad=5)
# ax.set_xlabel("w")
# ax.set_ylabel("b")
# ax.set_zlabel("cost (K)")
#ax.tick_params(axis='both', which='major', labelsize=6, pad=10)
#plt.tight_layout()

# w_index, b_index = np.where(costs == np.min(costs))
# ax.scatter(ws[w_index], bs[b_index], costs[w_index,b_index], color="red", s=40)
#print(np.min(costs))
#print(w_index, b_index)
#print(ws[w_index], bs[b_index])
#print(f"當ws{ws[w_index]}, bs{bs[b_index]} 會有最小cost:{costs[w_index,b_index]}")

# ax.scatter(w_hist[0], b_hist[0], c_hist[0], color="green", s=40)
# ax.plot(w_hist, b_hist, c_hist)

# plt.show()
#formula 1 = Math.pow(y - y_pred, 2)
#         = (y - y_pred) = (y - w*x ) = -2x(y-w*x)
#differential formula above
#keyword : learning rate, Slope, gradient descent
#bigger learning rate  bigger step
# 2*x*(w*x+b-y)